{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text analytics 1st Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
  
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Implement and train a bigram and a trigram language model\n",
    "Implement a bigram and a trigram language model for word sequences (e.g., sentences), using Laplace smoothing. Train your models on a training subset of a corpus (e.g., from the English part of Europarl). Include in the vocabulary only words that occur, e.g., at least 10 times in the training subset; use the same vocabulary in the bigram and trigram models. Replace all out-of-vocabular (OOV) words (in the training, development, test subsets) by a special token *UNK*. Assume that each sentence starts with the pseudo-token *start* (or the pseudo-tokens *start1*, *start2* for the trigram model) and ends with *end*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `corpus` used to both train and test the bigram and a trigram language models is from http://www.statmt.org/europarl/ (file: parallel corpus Greek-English, 145 MB, 04/1996-11/2011[only the English corpus is used]). It has 35,332,916 tokens in 1,307,128 sentences and 116,230 unique tokens. \n",
    "\n",
    "* The first `70%` of the total sentences was used as `training set`, the next `10%` was used as `development set` (fine-tuning) and the last `20%` of the sentences was used as `test set` to examine how well each model performs.\n",
    " \n",
    "* A `list` of all the tokens in the training set was built followed by a `counter` of these tokens.\n",
    "* Using the above-mentioned `counter` the  tokens that occur less than 10 times in the training set was removed from the counter.\n",
    "* The remaining key items of the counter was used as `vocabulary` for both models with size |V|=25,589 words.\n",
    "* Every token in each data set (train, development, test) which is not present in the vocabulary was replaced by the special token `*UNK*`.\n",
    "* All unigrams, bigrams and trigrams in the training set was counted. In order to build the bigrams counter it was assumed that each sentence starts with the pseudo-token `*start*` (or the pseudo-tokens `*start1*`, `*start2*` for the trigram model) and ends with `*end*`.\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/giorgosfat17/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "tweet_wt = TweetTokenizer()\n",
    "\n",
    "from random import sample,seed\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# read corpus txt\n",
    "with open (\"europarl-v7.el-en.txt\", \"r\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# make text a list of  sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# test, development and test sets\n",
    "train_sentences = sentences[:int(len(sentences)*0.7)]\n",
    "dev_sentences = sentences[int(len(sentences)*0.7):int(len(sentences)*0.8)]\n",
    "test_sentences = sentences[int(len(sentences)*0.8):]\n",
    "\n",
    "# test, development and test sets tokenazation\n",
    "train_sentences_tokenized = []\n",
    "for sent in train_sentences: \n",
    "    sent_tok = tweet_wt.tokenize(sent)\n",
    "    train_sentences_tokenized.append(sent_tok)\n",
    "    \n",
    "dev_sentences_tokenized = []\n",
    "for sent in dev_sentences:\n",
    "    sent_tok = tweet_wt.tokenize(sent)\n",
    "    dev_sentences_tokenized.append(sent_tok)\n",
    "\n",
    "test_sentences_tokenized = []\n",
    "for sent in test_sentences:\n",
    "    sent_tok = tweet_wt.tokenize(sent)\n",
    "    test_sentences_tokenized.append(sent_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "#tokens = tweet_wt.tokenize(text)\n",
    "\n",
    "# total tokens in train set\n",
    "train_tokens = []\n",
    "for sent in train_sentences_tokenized:\n",
    "    for tok in sent:\n",
    "        train_tokens.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# dictionary {token:number of occurances in train set}\n",
    "count = Counter(train_tokens)\n",
    "\n",
    "# remove tokens with less than 10 occurances\n",
    "count2 = {key:val for key, val in count.items() if val >= 10}\n",
    "\n",
    "vocabulary = list(set(count2.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Replace all out-of-vocabular (OOV) tokens \n",
    "for sent in train_sentences_tokenized:\n",
    "    for i in range(len(sent)):\n",
    "        if sent[i] not in count2.keys():\n",
    "            sent[i] = '*UNK*'\n",
    "        \n",
    "for sent in dev_sentences_tokenized:\n",
    "    for i in range(len(sent)):\n",
    "        if sent[i] not in count2.keys():\n",
    "            sent[i] = '*UNK*'\n",
    "            \n",
    "for sent in test_sentences_tokenized:\n",
    "    for i in range(len(sent)):\n",
    "        if sent[i] not in count2.keys():\n",
    "            sent[i] = '*UNK*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "unigram_counter = Counter()\n",
    "bigram_counter  = Counter()\n",
    "trigram_counter = Counter()\n",
    "\n",
    "# Train the models \n",
    "for sent in train_sentences_tokenized:\n",
    "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,\n",
    "                                                   left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    \n",
    "    bigram_counter.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True,\n",
    "                                                   left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    \n",
    "    trigram_counter.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True,\n",
    "                                                   left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidePrompt": false
   },
   "source": [
    "### ii) Probabilities  of correct VS incorrect unknown sentences based on the trained models\n",
    "Check the log-probabilities that your trained models return when given (correct) sentences from the test subset vs. (incorrect) sentences of the same length (in words) consisting of randomly selected vocabulary words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to check how well the trained language models perform, we will compare the probabilities of correct sentences versus incorrect ones (same but shuffled words) using Laplace smoothing.\n",
    "\n",
    "The log-probability of a given sentence with *k* words using bi-gram model with Laplace smoothing is given by this type:\n",
    "\n",
    "\n",
    "### $log_2P_{Bigram} = log_2\\frac{C(w_1|*start*) + a} { C(*start*)+ a|V|}+ log_2\\frac{C(w_2|w_1) + a} {C(w_1)+ a|V|}+ \\dots + log_2\\frac{C(*end*|w_k) + a} {C(w_k)+ a|V|}  $\n",
    "\n",
    "\n",
    "\n",
    "and the corresponding type for a tri-gram model is:\n",
    "\n",
    "\n",
    "### $log_2P_{Trigram} = log_2\\frac{C(w_1|*start1*,*start2*) + a} {C(*start1*,*start2*)+ a|V|}+ log_2\\frac{C(w_2|*start2*,w_1) + a} {C(*start2*,w_1)+ a|V|}+ \\dots + log_2\\frac{C(*end2*|w_k,*end1*) + a} {C(w_k,*end1*)+ a|V|}  $\n",
    "\n",
    "\n",
    "* $ C(w_1,w_2) $ : bigram count\n",
    "* $ C(w_1) $ : unigram count\n",
    "* $ 0 \\leq\\alpha \\leq1 $ :  smoothing hyper-parameter \n",
    "* |V|: vocabulary size\n",
    "\n",
    "\n",
    "* The outcomes of the above calculations shown that in both bi-gram and tri-gram models the probability of the correct sentence is by far higher than the one that derived from the incorrect sentence(same but shuffled words). Some outcomes of the above calculations are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# We should fine-tune alpha on a held-out dataset\n",
    "alpha = 0.01\n",
    "# Calculate vocab size \n",
    "vocab_size = len(set(vocabulary))\n",
    "\n",
    "''' functions to calculate the log probabilities of  a given correct sentence and an\n",
    "    incorrect one with Laplace smoothing, based on bigram and trigram models  \n",
    "'''\n",
    "\n",
    "def bigram_log_prob(sentence):\n",
    "    \n",
    "    seed(10)\n",
    "    bigram_log_prob = 0\n",
    "    \n",
    "    for token in range(len(sentence)-1):\n",
    "        bigram_log_prob += math.log2(bigram_counter[(sentence[token], sentence[token+1])] +alpha) / (unigram_counter[(sentence[token],)] + alpha*vocab_size)\n",
    "        \n",
    "        \n",
    "    bigram_log_prob_shuffled = 0\n",
    "    sentence2 = sample(vocabulary, len(sentence))\n",
    "    \n",
    "    for token in range(len(sentence2)-1):\n",
    "        bigram_log_prob_shuffled += math.log2(bigram_counter[(sentence2[token], sentence2[token+1])] +alpha) / (unigram_counter[(sentence2[token],)] + alpha*vocab_size)\n",
    "        \n",
    "    \n",
    "    return \"bigram_log_prob: {0:.3f}\".format(bigram_log_prob), \"bigram_log_prob_shuffled: {0:.3f}\".format(bigram_log_prob_shuffled) \n",
    "\n",
    "def trigram_log_prob(sentence):\n",
    "    \n",
    "    seed(10)\n",
    "    trigram_log_prob = 0\n",
    "    \n",
    "    for token in range(len(sentence)-2):\n",
    "        trigram_log_prob += math.log2(trigram_counter[(sentence[token], sentence[token+1],sentence[token+2])] +alpha) / (bigram_counter[(sentence[token],sentence[token+1])] + alpha*vocab_size)\n",
    "        \n",
    "        \n",
    "    trigram_log_prob_shuffled = 0\n",
    "    sentence2 = sample(vocabulary, len(sentence))\n",
    "    \n",
    "    for token in range(len(sentence2)-2):\n",
    "        trigram_log_prob_shuffled += math.log2(trigram_counter[(sentence2[token], sentence2[token+1],sentence2[token+2])] +alpha) / (bigram_counter[(sentence[token],sentence[token+1])] + alpha*vocab_size)\n",
    "        \n",
    "    \n",
    "    return \"trigram_log_prob: {0:.3f}\".format(trigram_log_prob), \"trigram_log_prob_shuffled: {0:.3f}\".format(trigram_log_prob_shuffled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Correct        VS      Incorrect Sentence\n",
      "Sentence: 1\n",
      "('bigram_log_prob: 0.011', 'bigram_log_prob_shuffled: -0.193')\n",
      "('trigram_log_prob: -0.029', 'trigram_log_prob_shuffled: -0.102')\n",
      "-----------------------------------------------------------------\n",
      "Sentence: 2\n",
      "('bigram_log_prob: 0.018', 'bigram_log_prob_shuffled: -0.305')\n",
      "('trigram_log_prob: -0.040', 'trigram_log_prob_shuffled: -0.184')\n",
      "-----------------------------------------------------------------\n",
      "Sentence: 3\n",
      "('bigram_log_prob: 0.032', 'bigram_log_prob_shuffled: -0.284')\n",
      "('trigram_log_prob: -0.010', 'trigram_log_prob_shuffled: -0.164')\n",
      "-----------------------------------------------------------------\n",
      "Sentence: 4\n",
      "('bigram_log_prob: 0.005', 'bigram_log_prob_shuffled: -0.427')\n",
      "('trigram_log_prob: -0.148', 'trigram_log_prob_shuffled: -0.298')\n",
      "-----------------------------------------------------------------\n",
      "Sentence: 5\n",
      "('bigram_log_prob: 0.023', 'bigram_log_prob_shuffled: -0.471')\n",
      "('trigram_log_prob: 0.035', 'trigram_log_prob_shuffled: -0.212')\n",
      "-----------------------------------------------------------------\n",
      "Sentence: 6\n",
      "('bigram_log_prob: 0.013', 'bigram_log_prob_shuffled: -0.284')\n",
      "('trigram_log_prob: 0.015', 'trigram_log_prob_shuffled: -0.140')\n",
      "-----------------------------------------------------------------\n",
      "Sentence: 7\n",
      "('bigram_log_prob: 0.024', 'bigram_log_prob_shuffled: -0.728')\n",
      "('trigram_log_prob: -0.033', 'trigram_log_prob_shuffled: -0.317')\n",
      "-----------------------------------------------------------------\n",
      "Sentence: 8\n",
      "('bigram_log_prob: 0.013', 'bigram_log_prob_shuffled: -0.218')\n",
      "('trigram_log_prob: 0.020', 'trigram_log_prob_shuffled: -0.099')\n",
      "-----------------------------------------------------------------\n",
      "Sentence: 9\n",
      "('bigram_log_prob: 0.035', 'bigram_log_prob_shuffled: -0.528')\n",
      "('trigram_log_prob: 0.041', 'trigram_log_prob_shuffled: -0.267')\n",
      "-----------------------------------------------------------------\n",
      "Sentence: 10\n",
      "('bigram_log_prob: 0.003', 'bigram_log_prob_shuffled: -0.404')\n",
      "('trigram_log_prob: -0.161', 'trigram_log_prob_shuffled: -0.288')\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"         Correct        VS      Incorrect Sentence\")\n",
    "for i in range(10):\n",
    "    print(\"Sentence:\", i+1)\n",
    "    print(bigram_log_prob(test_sentences_tokenized[i]))\n",
    "    print(trigram_log_prob(test_sentences_tokenized[i]))\n",
    "    print('-----------------------------------------------------------------')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Estimate the language cross-entropy and perplexity of your models on the test set\n",
    "Estimate the language cross-entropy and perplexity of your models on the test subset of the corpus, treating the entire test subset as a single sequence, with *start* (or *start1*, *start2*) at the beginning of each sentence, and *end* at the end of each sentence. Do not include probabilities of the form P(*start*|…) (or P(*start1*|…) or P(*start2*|…)) in the computation of perplexity, but include probabilities of the form P(*end*|…)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross Entropy = $ -\\frac{\\sum{log_2P_{n-gram}}}{N_{n-gram}}$\n",
    "* Perplexity = $ 2^{Cross Entropy}$\n",
    "* First we fine tune `alpha` hyperparameter in development set both for bigram and trigram model. As it can been seen below the optimum value for alpha for both models is `0.01`.\n",
    "\n",
    "\n",
    "* Bigram model:\n",
    "    - Cross Entropy: 6.743\n",
    "    - Perplexity   : 107.130\n",
    "* Trigram model:\n",
    "    - Cross Entropy: 7.567\n",
    "    - Perplexity   : 189.657\n",
    "    \n",
    "* From the results we can see that the Bigram model performs way better in the Test set than the Trigram, meaning that for this corpus the Trigram model overfits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tune alpha for Bi-gram Model\n",
      "--------------\n",
      "Alpha:0.01\n",
      "Cross Entropy: 6.741\n",
      "perplexity: 106.995\n",
      "--------------\n",
      "Alpha:0.03\n",
      "Cross Entropy: 6.825\n",
      "perplexity: 113.383\n",
      "--------------\n",
      "Alpha:0.05\n",
      "Cross Entropy: 6.897\n",
      "perplexity: 119.151\n",
      "--------------\n",
      "Alpha:0.07\n",
      "Cross Entropy: 6.958\n",
      "perplexity: 124.291\n",
      "--------------\n",
      "Alpha:0.09\n",
      "Cross Entropy: 7.011\n",
      "perplexity: 128.971\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# fine tune alpha for bi-gram\n",
    "alpha = np.arange(0.01,0.1,.02)\n",
    "print(\"Fine tune alpha for Bi-gram Model\")\n",
    "print(\"--------------\")\n",
    "for alpha in alpha:\n",
    "    sum_prob = 0\n",
    "    bigram_cnt = 0\n",
    "    for sent in dev_sentences_tokenized:\n",
    "        sent = sent + ['*end*']\n",
    "        for i in range(len(sent)-1):\n",
    "            bigram_prob = (bigram_counter[(sent[i], sent[i+1])] +alpha) / (unigram_counter[(sent[i],)] + alpha*vocab_size)\n",
    "            sum_prob += math.log2(bigram_prob)\n",
    "            bigram_cnt+=1\n",
    "\n",
    "    HC = -sum_prob / bigram_cnt\n",
    "    perpl = math.pow(2,HC)\n",
    "   \n",
    "    print(\"Alpha:{0:.2f}\".format(alpha))\n",
    "    print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
    "    print(\"perplexity: {0:.3f}\".format(perpl))\n",
    "    print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tune alpha for Tri-gram Model\n",
      "--------------\n",
      "Alpha:0.01\n",
      "Cross Entropy: 7.546\n",
      "perplexity: 186.899\n",
      "--------------\n",
      "Alpha:0.03\n",
      "Cross Entropy: 7.936\n",
      "perplexity: 244.898\n",
      "--------------\n",
      "Alpha:0.05\n",
      "Cross Entropy: 8.178\n",
      "perplexity: 289.627\n",
      "--------------\n",
      "Alpha:0.07\n",
      "Cross Entropy: 8.358\n",
      "perplexity: 328.037\n",
      "--------------\n",
      "Alpha:0.09\n",
      "Cross Entropy: 8.502\n",
      "perplexity: 362.512\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# fine tune alpha for tri-gram\n",
    "alpha = np.arange(0.01,0.1,.02)\n",
    "print(\"Fine tune alpha for Tri-gram Model\")\n",
    "print(\"--------------\")\n",
    "for alpha in alpha:\n",
    "    sum_prob = 0\n",
    "    trigram_cnt = 0\n",
    "    for sent in dev_sentences_tokenized:\n",
    "        sent =  sent +  ['*end*'] + ['*end*']\n",
    "        for i in range(len(sent)-2):\n",
    "            trigram_prob = (trigram_counter[(sent[i], sent[i+1],sent[i+2])] +alpha) / (bigram_counter[(sent[i],sent[i+1])] + alpha*vocab_size)\n",
    "            sum_prob += math.log2(trigram_prob)\n",
    "            trigram_cnt+=1\n",
    "\n",
    "    HC = -sum_prob / trigram_cnt\n",
    "    perpl = math.pow(2,HC)\n",
    "    print(\"Alpha:{0:.2f}\".format(alpha))\n",
    "    print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
    "    print(\"perplexity: {0:.3f}\".format(perpl))\n",
    "    print(\"--------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Thus, we use alpha = 0.01 as Laplace smoothing hyper-parameter in order to evaluate the two models on the unknown Test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-gram performance on Test set:\n",
      "Cross Entropy: 6.743\n",
      "perplexity: 107.130\n"
     ]
    }
   ],
   "source": [
    "# bi-gram model on test set\n",
    "alpha = 0.01\n",
    "sum_prob = 0\n",
    "bigram_cnt = 0\n",
    "for sent in test_sentences_tokenized:\n",
    "    sent = sent + ['*end*']\n",
    "    for i in range(len(sent)-1):\n",
    "        bigram_prob = (bigram_counter[(sent[i], sent[i+1])] + alpha) / (unigram_counter[(sent[i],)] + alpha*vocab_size)\n",
    "        sum_prob += math.log2(bigram_prob)\n",
    "        bigram_cnt+=1\n",
    "\n",
    "HC = -sum_prob / bigram_cnt\n",
    "perpl = math.pow(2,HC)\n",
    "print(\"Bi-gram performance on Test set:\")\n",
    "print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"perplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tri-gram performance on Test set:\n",
      "Cross Entropy: 7.567\n",
      "perplexity: 189.657\n"
     ]
    }
   ],
   "source": [
    "# tri-gram model on test set\n",
    "sum_prob = 0\n",
    "trigram_cnt = 0\n",
    "for sent in test_sentences_tokenized:\n",
    "    sent =  sent + ['*end*'] + ['*end*']\n",
    "    for i in range(len(sent)-2):\n",
    "        trigram_prob = (trigram_counter[(sent[i], sent[i+1],sent[i+2])] + alpha) / (bigram_counter[(sent[i],sent[i+1])] + alpha*vocab_size)\n",
    "        sum_prob += math.log2(trigram_prob)\n",
    "        trigram_cnt+=1\n",
    "\n",
    "HC = -sum_prob / trigram_cnt\n",
    "perpl = math.pow(2,HC)\n",
    "print(\"Tri-gram performance on Test set:\")\n",
    "print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"perplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv) Interpolated bi-gram and tri-gram LM\n",
    "\n",
    "* We will combine the two models using linear interpolation and check if the combined model performs better.\n",
    "### $ P(w_3|w_1,w_2) = \\lambda \\cdot P(w_3|w_1,w_2) +(1-\\lambda) \\cdot P(w_3|w_2)  $\n",
    "\n",
    "\n",
    "* $ 0 \\leq \\lambda \\leq 1 $\n",
    "* We will calculate the cross entropy for several values of $ \\lambda $ in order to find out if the interpolated model performs better than the bi-gram. For $ \\lambda $=0 the interpolated model takes into account only the bi-gram, while for  $\\lambda $=1  only the tri-gram is taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:0.00\n",
      "Cross Entropy: 6.741\n",
      "perplexity: 106.995\n",
      "---------------\n",
      "Lamda:0.10\n",
      "Cross Entropy: 6.822\n",
      "perplexity: 113.132\n",
      "---------------\n",
      "Lamda:0.20\n",
      "Cross Entropy: 6.902\n",
      "perplexity: 119.622\n",
      "---------------\n",
      "Lamda:0.30\n",
      "Cross Entropy: 6.983\n",
      "perplexity: 126.484\n",
      "---------------\n",
      "Lamda:0.40\n",
      "Cross Entropy: 7.063\n",
      "perplexity: 133.740\n",
      "---------------\n",
      "Lamda:0.50\n",
      "Cross Entropy: 7.144\n",
      "perplexity: 141.412\n",
      "---------------\n",
      "Lamda:0.60\n",
      "Cross Entropy: 7.224\n",
      "perplexity: 149.523\n",
      "---------------\n",
      "Lamda:0.70\n",
      "Cross Entropy: 7.305\n",
      "perplexity: 158.101\n",
      "---------------\n",
      "Lamda:0.80\n",
      "Cross Entropy: 7.385\n",
      "perplexity: 167.170\n",
      "---------------\n",
      "Lamda:0.90\n",
      "Cross Entropy: 7.466\n",
      "perplexity: 176.760\n",
      "---------------\n",
      "Lamda:1.00\n",
      "Cross Entropy: 7.546\n",
      "perplexity: 186.899\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# fine-tune lamda on development set\n",
    "lamda = np.arange(0,1.1,0.1)\n",
    "for lamda in lamda:\n",
    "    sum_prob = 0\n",
    "    ngram_cnt = 0\n",
    "    for sent in dev_sentences_tokenized:\n",
    "        sent =  sent + ['*end*'] + ['*end*']\n",
    "        for i in range(len(sent)-2):\n",
    "            trigram_prob = (trigram_counter[(sent[i], sent[i+1],sent[i+2])] + alpha) / (bigram_counter[(sent[i],sent[i+1])] + alpha*vocab_size)\n",
    "            bigram_prob = (bigram_counter[(sent[i], sent[i+1])] + alpha) / (unigram_counter[(sent[i],)] + alpha*vocab_size)\n",
    "            \n",
    "            sum_prob += (lamda * math.log2(trigram_prob)) + ((1-lamda) * math.log2(bigram_prob))\n",
    "            ngram_cnt+=1 \n",
    "\n",
    "    HC = -sum_prob / ngram_cnt\n",
    "    perpl = math.pow(2,HC)\n",
    "    print(\"Lamda:{0:.2f}\".format(lamda))\n",
    "    print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
    "    print(\"perplexity: {0:.3f}\".format(perpl))\n",
    "    print(\"---------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the above results on the development set, it can be seen that as the value of $\\lambda$ raises, meaning that the tri-gram model is taken more and more into account, cross entropy raises too. Therefore, the combined model does not perform better than the bi-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
